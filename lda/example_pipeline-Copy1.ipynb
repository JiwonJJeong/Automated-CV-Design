{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated CV Design: Complete Pipeline Example\n",
    "\n",
    "This notebook demonstrates a complete workflow for processing pairwise distance data, including:\n",
    "1. **Data Loading**: Using the refactored `data_access` to load H5/NPY files\n",
    "2. **Variance Filtering**: Removing low-variance features using knee-point method\n",
    "3. **Class Assignment**: Assigning 8 classes based on construct Ã— subconstruct combinations\n",
    "4. **Feature Selection**: Multiple methods (Chi-Squared, Fisher-AMINO, MPSO, BPSO)\n",
    "5. **Dimensionality Reduction**: Multiple methods (FLDA, GDHLDA, MHLDA, ZHLDA, PCA)\n",
    "\n",
    "## Data Structure\n",
    "- **Constructs**: `calmodulin`, `calmodulin-compact` (2 constructs)\n",
    "- **Subconstructs**: `ca-mg-1-2`, `ca-mg-1-4`, `ca-only`, `mg-only` (4 subconstructs each)\n",
    "- **Total Classes**: 2 Ã— 4 = 8 classes\n",
    "\n",
    "## Pipeline Combinations\n",
    "This framework supports all 20 combinations:\n",
    "- **Variance**: 1 method\n",
    "- **Feature Selection**: 4 methods (Chi-Squared, Fisher-AMINO, MPSO, BPSO)\n",
    "- **Dimensionality Reduction**: 5 methods (FLDA, GDHLDA, MHLDA, ZHLDA, PCA)\n",
    "- **Total**: 1 Ã— 4 Ã— 5 = 20 pipeline combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add the lda directory to Python path\n",
    "lda_path = os.path.join(os.getcwd(), 'lda')\n",
    "if lda_path not in sys.path:\n",
    "    sys.path.append(lda_path)\n",
    "\n",
    "# Import the interactive pipeline functions\n",
    "from pipeline_helper import run_interactive_pipeline, create_interactive_pipeline_configs\n",
    "\n",
    "print(\"âœ… Imports completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports completed successfully!\n",
      "ğŸ“Š Available data:\n",
      "ğŸ—ï¸  Constructs: ['calmodulin', 'calmodulin-zeb-bp2', 'calmodulin-compact']\n",
      "ğŸ”§ Subconstructs: ['ca-mg-1-4', 'mg-only', 'ca-mg-1-2', 'ca-only']\n",
      "   ğŸ“‹ calmodulin: ['ca-mg-1-2', 'ca-mg-1-4', 'ca-only', 'mg-only']\n",
      "   ğŸ“‹ calmodulin-zeb-bp2: ['ca-mg-1-2', 'ca-mg-1-4', 'ca-only', 'mg-only']\n",
      "   ğŸ“‹ calmodulin-compact: ['ca-mg-1-2', 'ca-mg-1-4', 'ca-only', 'mg-only']\n",
      "Loaded 144 canonical residues from /work/hdd/bfri/jjeong7/analysis_output/dist_maps/calmodulin/ca-mg-1-2/canonical_resids.npy\n",
      "\n",
      "ğŸ“Š Test data shape: (3000, 10301)\n",
      "ğŸ—ï¸  Constructs: ['calmodulin']\n",
      "ğŸ”§ Subconstructs: ['ca-mg-1-2']\n",
      "ğŸ“‹ Feature columns: 10296\n",
      "ğŸ“‹ Sample columns: ['RES2_3', 'RES2_4', 'RES2_5', 'RES2_6', 'RES2_7', 'RES2_8', 'RES2_9', 'RES2_10', 'RES2_11', 'RES2_12']...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add the lda directory to Python path\n",
    "lda_path = os.path.join(os.getcwd(), 'lda')\n",
    "if lda_path not in sys.path:\n",
    "    sys.path.append(lda_path)\n",
    "\n",
    "# Import the interactive pipeline functions\n",
    "from pipeline_helper import run_interactive_pipeline, create_interactive_pipeline_configs\n",
    "\n",
    "print(\"âœ… Imports completed successfully!\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 2: USE YOUR EXISTING DATA ACCESS\n",
    "# =============================================================================\n",
    "# Import your existing data access module\n",
    "from data_access import create_dataframe_factory, list_available_constructs_subconstructs\n",
    "\n",
    "# Set the correct data directory for your system\n",
    "# Update the BASE_DIR to point to your actual data location\n",
    "import data_access\n",
    "data_dir = '/work/hdd/bfri/jjeong7/analysis_output/dist_maps'\n",
    "constructs_dict, subconstructs_dict = list_available_constructs_subconstructs(base_dir=data_dir)\n",
    "\n",
    "print(\"ğŸ“Š Available data:\")\n",
    "print(f\"ğŸ—ï¸  Constructs: {list(constructs_dict.keys())}\")\n",
    "print(f\"ğŸ”§ Subconstructs: {list(subconstructs_dict.keys())}\")\n",
    "\n",
    "# Show details for each construct\n",
    "for construct, subconstructs in constructs_dict.items():\n",
    "    print(f\"   ğŸ“‹ {construct}: {subconstructs}\")\n",
    "\n",
    "# Create your data factory WITHOUT the class column\n",
    "# The pipeline runner will inject it later using the callback\n",
    "data_factory = create_dataframe_factory(base_dir=data_dir)\n",
    "\n",
    "# Test the data factory\n",
    "test_data = next(data_factory())\n",
    "print(f\"\\nğŸ“Š Test data shape: {test_data.shape}\")\n",
    "print(f\"ğŸ—ï¸  Constructs: {test_data['construct'].unique()}\")\n",
    "print(f\"ğŸ”§ Subconstructs: {test_data['subconstruct'].unique()}\")\n",
    "print(f\"ğŸ“‹ Feature columns: {len([col for col in test_data.columns if col not in ['construct', 'subconstruct', 'replica', 'frame_number', 'time']])}\")\n",
    "\n",
    "# Show a few columns to understand the data structure\n",
    "print(f\"ğŸ“‹ Sample columns: {test_data.columns.tolist()[:10]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Generated 20 pipeline configurations\n",
      "ğŸ“Š Feature selection methods: 4 (bpso, mpso, fisher_amino, chi_sq_amino)\n",
      "ğŸ¯ Dimensionality reduction methods: 5 (flda, pca, zhlda, mhlda, gdhlda)\n",
      "ğŸ“ˆ Total combinations: 4 Ã— 5 = 20\n",
      "\n",
      "ğŸ“‹ First 5 pipeline configurations:\n",
      "   1. bpso_to_flda\n",
      "   2. bpso_to_pca\n",
      "   3. bpso_to_zhlda\n",
      "   4. bpso_to_mhlda\n",
      "   5. bpso_to_gdhlda\n"
     ]
    }
   ],
   "source": [
    "# Create all possible pipeline combinations\n",
    "configs = create_interactive_pipeline_configs()\n",
    "\n",
    "print(f\"ğŸ”§ Generated {len(configs)} pipeline configurations\")\n",
    "print(f\"ğŸ“Š Feature selection methods: 4 (bpso, mpso, fisher_amino, chi_sq_amino)\")\n",
    "print(f\"ğŸ¯ Dimensionality reduction methods: 5 (flda, pca, zhlda, mhlda, gdhlda)\")\n",
    "print(f\"ğŸ“ˆ Total combinations: 4 Ã— 5 = 20\")\n",
    "\n",
    "# Show first few configurations\n",
    "print(\"\\nğŸ“‹ First 5 pipeline configurations:\")\n",
    "for i, config in enumerate(configs[:5]):\n",
    "    print(f\"   {i+1}. {config['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Interactive Pipeline Runner\n",
      "ğŸ’¡ Variance runs first, then feature selection, then dimensionality reduction\n",
      "âš™ï¸  You'll set parameters for each phase\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Found cached result for VARIANCE (pipeline_cache/variance.pkl). Load? (Y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached VARIANCE data.\n",
      "Shape: (119991, 274)\n",
      "\n",
      "[FEATURE_SELECTION : CHI_SQ_AMINO]\n",
      "  max_amino: 10\n",
      "  q_bins: 5\n",
      "  knee_S: 5.0\n",
      "  sample_rows: 20000\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Modify? (y/N):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CHI_SQ_AMINO...\n",
      "Successfully imported AMINO from: /u/jjeong7/ondemand/lda/feature_selection/amino.py\n",
      "Sampling to estimate Chi-Squared bin edges and classes...\n",
      "Building Chi-Squared scores sequentially...\n",
      "Knee Point: 16856.4120 | Candidates: 29\n",
      "Loading 29 candidate features into memory for AMINO...\n",
      "Running AMINO: Asking for 10 features from 29 candidates...\n",
      "Selected bandwidth: 0.3132549209074307\n",
      "\n",
      "Checking 10 order parameters...\n",
      "Error executing CHI_SQ_AMINO: 'NoneType' object cannot be interpreted as an integer\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ Starting Interactive Pipeline Runner\")\n",
    "print(\"ğŸ’¡ Variance runs first, then feature selection, then dimensionality reduction\")\n",
    "print(\"âš™ï¸  You'll set parameters for each phase\")\n",
    "\n",
    "# Use the pipeline runner\n",
    "results = run_interactive_pipeline(data_factory, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ğŸ“Š Successful pipelines: {len(results)}\")\n",
    "\n",
    "if results:\n",
    "    print(\"\\nğŸ“ˆ Results Summary:\")\n",
    "    for pipeline_name, result in results.items():\n",
    "        final_df = result['final_result']\n",
    "        feature_cols = [col for col in final_df.columns if col != 'class']\n",
    "        \n",
    "        print(f\"   âœ… {pipeline_name}:\")\n",
    "        print(f\"      ğŸ“ Shape: {final_df.shape}\")\n",
    "        print(f\"      ğŸ”§ Features: {len(feature_cols)}\")\n",
    "        print(f\"      ğŸ·ï¸  Classes: {final_df['class'].nunique()}\")\n",
    "        \n",
    "        # Show feature names if not too many\n",
    "        if len(feature_cols) <= 5:\n",
    "            print(f\"      ğŸ“‹ Features: {feature_cols}\")\n",
    "        else:\n",
    "            print(f\"      ğŸ“‹ Features: {feature_cols[:3]}...{feature_cols[-2:]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âš ï¸  No pipelines completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    import pickle\n",
    "    import datetime\n",
    "    \n",
    "    # Create timestamp for filename\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"pipeline_results_{timestamp}.pkl\"\n",
    "    \n",
    "    # Save results to file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    print(f\"ğŸ’¾ Results saved to: {results_file}\")\n",
    "    \n",
    "    # Also save summary to CSV\n",
    "    summary_data = []\n",
    "    for pipeline_name, result in results.items():\n",
    "        final_df = result['final_result']\n",
    "        summary_data.append({\n",
    "            'pipeline': pipeline_name,\n",
    "            'samples': final_df.shape[0],\n",
    "            'features': len([col for col in final_df.columns if col != 'class']),\n",
    "            'classes': final_df['class'].nunique(),\n",
    "            'feature_selection': result['config']['feature_selection'],\n",
    "            'dimensionality_reduction': result['config']['dimensionality_reduction']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_file = f\"pipeline_summary_{timestamp}.csv\"\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"ğŸ“Š Summary saved to: {summary_file}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No results to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
