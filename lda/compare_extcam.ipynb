{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of CaM extended\n",
    "## Comparison of all CaM states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add the lda directory to Python path\n",
    "lda_path = os.path.join(os.getcwd(), 'lda')\n",
    "if lda_path not in sys.path:\n",
    "    sys.path.append(lda_path)\n",
    "\n",
    "# Import the interactive pipeline functions\n",
    "from pipeline_helper import run_interactive_pipeline, create_interactive_pipeline_configs\n",
    "\n",
    "# Import your existing data access module\n",
    "from data_access import create_dataframe_factory, list_available_constructs_subconstructs\n",
    "\n",
    "print(\"‚úÖ Imports completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/work/hdd/bfri/jjeong7/analysis_output/dist_maps'\n",
    "constructs_dict, subconstructs_dict = list_available_constructs_subconstructs(base_dir=data_dir)\n",
    "\n",
    "# Specific states of specific proteins\n",
    "data_factory = create_dataframe_factory(\n",
    "    base_dir=data_dir, \n",
    "    constructs=['calmodulin']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all possible pipeline combinations\n",
    "configs = create_interactive_pipeline_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Interactive Pipeline Runner\n",
      "üí° Variance runs first, then feature selection, then dimensionality reduction\n",
      "‚öôÔ∏è  You'll set parameters for each phase\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Found cached result for VARIANCE (pipeline_cache/variance.pkl). Load? (Y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached VARIANCE data.\n",
      "Shape: (90000, 509)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Found cached result for CHI_SQ_AMINO (pipeline_cache/chi_sq_amino.pkl). Load? (Y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached CHI_SQ_AMINO data.\n",
      "Shape: (18000, 3)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Found cached result for FISHER_AMINO (pipeline_cache/fisher_amino.pkl). Load? (Y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached FISHER_AMINO data.\n",
      "Shape: (90000, 3)\n",
      "\n",
      "[FEATURE_SELECTION : MPSO]\n",
      "  population_size: 20\n",
      "  mpso_iters: 50\n",
      "  alpha: 0.9\n",
      "  threshold: 0.5\n",
      "  stride: 5\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Modify? (y/N):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MPSO...\n",
      "Pass 1: Computing Fisher scores (stride=5)...\n",
      "Pass 2: Loading top 250 features (stride=5)...\n",
      "Running MPSO on 18000 strided samples...\n",
      "Beginning Swarm Optimization...\n",
      "‚úÖ MPSO Complete. Reduced 250 features to 5 dimensions.\n",
      "MPSO Result Shape: (18000, 8)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Accept MPSO results? (y/N):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results accepted and cached to pipeline_cache/mpso.pkl\n",
      "\n",
      "[FEATURE_SELECTION : BPSO]\n",
      "  population_size: 20\n",
      "  max_iters: 50\n",
      "  w: 0.729\n",
      "  c1: 1.49445\n",
      "  c2: 1.49445\n",
      "  stride: 5\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Modify? (y/N):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BPSO...\n",
      "Pass 1: Filtering features via Streaming Fisher Score (stride=5)...\n",
      "Pass 2: Loading top 150 features (stride=5)...\n",
      "Beginning Swarm Optimization on 18000 samples...\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Starting Interactive Pipeline Runner\")\n",
    "print(\"üí° Variance runs first, then feature selection, then dimensionality reduction\")\n",
    "print(\"‚öôÔ∏è  You'll set parameters for each phase\")\n",
    "\n",
    "# Use the pipeline runner\n",
    "results = run_interactive_pipeline(data_factory, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìä Successful pipelines: {len(results)}\")\n",
    "\n",
    "if results:\n",
    "    print(\"\\nüìà Results Summary:\")\n",
    "    for pipeline_name, result in results.items():\n",
    "        final_df = result['final_result']\n",
    "        feature_cols = [col for col in final_df.columns if col != 'class']\n",
    "        \n",
    "        print(f\"   ‚úÖ {pipeline_name}:\")\n",
    "        print(f\"      üìè Shape: {final_df.shape}\")\n",
    "        print(f\"      üîß Features: {len(feature_cols)}\")\n",
    "        print(f\"      üè∑Ô∏è  Classes: {final_df['class'].nunique()}\")\n",
    "        \n",
    "        # Show feature names if not too many\n",
    "        if len(feature_cols) <= 5:\n",
    "            print(f\"      üìã Features: {feature_cols}\")\n",
    "        else:\n",
    "            print(f\"      üìã Features: {feature_cols[:3]}...{feature_cols[-2:]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No pipelines completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    import pickle\n",
    "    import datetime\n",
    "    \n",
    "    # Create timestamp for filename\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = f\"pipeline_results_{timestamp}.pkl\"\n",
    "    \n",
    "    # Save results to file\n",
    "    with open(results_file, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    print(f\"üíæ Results saved to: {results_file}\")\n",
    "    \n",
    "    # Also save summary to CSV\n",
    "    summary_data = []\n",
    "    for pipeline_name, result in results.items():\n",
    "        final_df = result['final_result']\n",
    "        summary_data.append({\n",
    "            'pipeline': pipeline_name,\n",
    "            'samples': final_df.shape[0],\n",
    "            'features': len([col for col in final_df.columns if col != 'class']),\n",
    "            'classes': final_df['class'].nunique(),\n",
    "            'feature_selection': result['config']['feature_selection'],\n",
    "            'dimensionality_reduction': result['config']['dimensionality_reduction']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_file = f\"pipeline_summary_{timestamp}.csv\"\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"üìä Summary saved to: {summary_file}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No results to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
