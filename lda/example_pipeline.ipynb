{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Automated CV Design: Complete Pipeline Example\n",
                "\n",
                "This notebook demonstrates a complete workflow for processing pairwise distance data, including:\n",
                "1. **Data Loading**: Using the refactored `data_access` to load H5/NPY files\n",
                "2. **Variance Filtering**: Removing low-variance features using knee-point method\n",
                "3. **Class Assignment**: Assigning 8 classes based on construct Ã— subconstruct combinations\n",
                "4. **Feature Selection**: Multiple methods (Chi-Squared, Fisher-AMINO, MPSO, BPSO)\n",
                "5. **Dimensionality Reduction**: Multiple methods (FLDA, GDHLDA, MHLDA, ZHLDA, PCA)\n",
                "\n",
                "## Data Structure\n",
                "- **Constructs**: `calmodulin`, `calmodulin-compact` (2 constructs)\n",
                "- **Subconstructs**: `ca-mg-1-2`, `ca-mg-1-4`, `ca-only`, `mg-only` (4 subconstructs each)\n",
                "- **Total Classes**: 2 Ã— 4 = 8 classes\n",
                "\n",
                "## Pipeline Combinations\n",
                "This framework supports all 20 combinations:\n",
                "- **Variance**: 1 method\n",
                "- **Feature Selection**: 4 methods (Chi-Squared, Fisher-AMINO, MPSO, BPSO)\n",
                "- **Dimensionality Reduction**: 5 methods (FLDA, GDHLDA, MHLDA, ZHLDA, PCA)\n",
                "- **Total**: 1 Ã— 4 Ã— 5 = 20 pipeline combinations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Imports completed successfully!\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Add the lda directory to Python path\n",
                "lda_path = os.path.join(os.getcwd(), 'lda')\n",
                "if lda_path not in sys.path:\n",
                "    sys.path.append(lda_path)\n",
                "\n",
                "# Import the interactive pipeline functions\n",
                "from pipeline_helper import run_interactive_pipeline, create_interactive_pipeline_configs\n",
                "\n",
                "print(\"âœ… Imports completed successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Imports completed successfully!\n",
                        "ğŸ“Š Available data:\n",
                        "ğŸ—ï¸  Constructs: ['calmodulin-compact', 'calmodulin']\n",
                        "ğŸ”§ Subconstructs: ['mg-only', 'ca-mg-1-4', 'ca-mg-1-2', 'ca-only']\n",
                        "   ğŸ“‹ calmodulin-compact: ['ca-mg-1-2', 'ca-mg-1-4', 'ca-only', 'mg-only']\n",
                        "   ğŸ“‹ calmodulin: ['ca-mg-1-2', 'ca-mg-1-4', 'ca-only', 'mg-only']\n",
                        "Loaded 144 canonical residues from /home/jiwonjjeong/gk-lab/Automated-CV-Design/data/dist_maps/calmodulin/ca-mg-1-2/canonical_resids.npy\n",
                        "Warning: Time array length mismatch detected. Using sequential time values.\n",
                        "\n",
                        "ğŸ“Š Test data shape: (3000, 10302)\n",
                        "ğŸ—ï¸  Constructs: ['calmodulin']\n",
                        "ğŸ”§ Subconstructs: ['ca-mg-1-2']\n",
                        "ï¸  Classes: ['calmodulin_ca-mg-1-2']\n",
                        "ğŸ“‹ Feature columns: 10296\n",
                        "ğŸ“‹ Sample columns: ['RES2_3', 'RES2_4', 'RES2_5', 'RES2_6', 'RES2_7', 'RES2_8', 'RES2_9', 'RES2_10', 'RES2_11', 'RES2_12']...\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Add the lda directory to Python path\n",
                "lda_path = os.path.join(os.getcwd(), 'lda')\n",
                "if lda_path not in sys.path:\n",
                "    sys.path.append(lda_path)\n",
                "\n",
                "# Import the interactive pipeline functions\n",
                "from pipeline_helper import run_interactive_pipeline, create_interactive_pipeline_configs\n",
                "\n",
                "print(\"âœ… Imports completed successfully!\")\n",
                "\n",
                "# =============================================================================\n",
                "# CELL 2: USE YOUR EXISTING DATA ACCESS\n",
                "# =============================================================================\n",
                "# Import your existing data access module\n",
                "from data_access import create_dataframe_factory, list_available_constructs_subconstructs\n",
                "\n",
                "# Set the correct data directory for your system\n",
                "# Update the BASE_DIR to point to your actual data location\n",
                "import data_access\n",
                "data_access.BASE_DIR = \"/home/jiwonjjeong/gk-lab/Automated-CV-Design/data/dist_maps\"\n",
                "\n",
                "# Check what data is available\n",
                "data_dir = \"/home/jiwonjjeong/gk-lab/Automated-CV-Design/data/dist_maps\"\n",
                "constructs_dict, subconstructs_dict = list_available_constructs_subconstructs(base_dir=data_dir)\n",
                "\n",
                "print(\"ğŸ“Š Available data:\")\n",
                "print(f\"ğŸ—ï¸  Constructs: {list(constructs_dict.keys())}\")\n",
                "print(f\"ğŸ”§ Subconstructs: {list(subconstructs_dict.keys())}\")\n",
                "\n",
                "# Show details for each construct\n",
                "for construct, subconstructs in constructs_dict.items():\n",
                "    print(f\"   ğŸ“‹ {construct}: {subconstructs}\")\n",
                "\n",
                "# Create your data factory WITHOUT the class column\n",
                "# The pipeline runner will inject it later using the callback\n",
                "data_factory = create_dataframe_factory(base_dir=data_dir)\n",
                "\n",
                "# Test the data factory\n",
                "test_data = next(data_factory())\n",
                "print(f\"\\nğŸ“Š Test data shape: {test_data.shape}\")\n",
                "print(f\"ğŸ—ï¸  Constructs: {test_data['construct'].unique()}\")\n",
                "print(f\"ğŸ”§ Subconstructs: {test_data['subconstruct'].unique()}\")\n",
                "print(f\"ğŸ“‹ Feature columns: {len([col for col in test_data.columns if col not in ['construct', 'subconstruct', 'replica', 'frame_number', 'time']])}\")\n",
                "\n",
                "# Show a few columns to understand the data structure\n",
                "print(f\"ğŸ“‹ Sample columns: {test_data.columns.tolist()[:10]}...\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ”§ Generated 20 pipeline configurations\n",
                        "ğŸ“Š Feature selection methods: 4 (bpso, mpso, fisher_amino, chi_sq_amino)\n",
                        "ğŸ¯ Dimensionality reduction methods: 5 (flda, pca, zhlda, mhlda, gdhlda)\n",
                        "ğŸ“ˆ Total combinations: 4 Ã— 5 = 20\n",
                        "\n",
                        "ğŸ“‹ First 5 pipeline configurations:\n",
                        "   1. bpso_to_flda\n",
                        "   2. bpso_to_pca\n",
                        "   3. bpso_to_zhlda\n",
                        "   4. bpso_to_mhlda\n",
                        "   5. bpso_to_gdhlda\n"
                    ]
                }
            ],
            "source": [
                "# Create all possible pipeline combinations\n",
                "configs = create_interactive_pipeline_configs()\n",
                "\n",
                "print(f\"ğŸ”§ Generated {len(configs)} pipeline configurations\")\n",
                "print(f\"ğŸ“Š Feature selection methods: 4 (bpso, mpso, fisher_amino, chi_sq_amino)\")\n",
                "print(f\"ğŸ¯ Dimensionality reduction methods: 5 (flda, pca, zhlda, mhlda, gdhlda)\")\n",
                "print(f\"ğŸ“ˆ Total combinations: 4 Ã— 5 = 20\")\n",
                "\n",
                "# Show first few configurations\n",
                "print(\"\\nğŸ“‹ First 5 pipeline configurations:\")\n",
                "for i, config in enumerate(configs[:5]):\n",
                "    print(f\"   {i+1}. {config['name']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸš€ Starting Interactive Pipeline Runner\n",
                        "ğŸ’¡ Variance runs first, then feature selection, then dimensionality reduction\n",
                        "âš™ï¸  You'll set parameters for each phase\n",
                        "\n",
                        "==============================\n",
                        "PHASE 1: VARIANCE\n",
                        "==============================\n",
                        "\n",
                        "[VARIANCE]\n",
                        "  show_plot: False\n",
                        "  knee_S: 1.0\n",
                        "  outlier_multiplier: 3.0\n",
                        "  fallback_percentile: 90\n",
                        "  min_clean_ratio: 0.5\n",
                        "  plot_pause: 3.0\n",
                        "Pass 1: Analyzing feature variance...\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâš™ï¸  You\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll set parameters for each phase\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Use the pipeline runner\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_interactive_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/gk-lab/Automated-CV-Design/lda/pipeline_helper.py:87\u001b[0m, in \u001b[0;36mrun_interactive_pipeline\u001b[0;34m(data_factory, pipeline_configs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Run variance filtering immediately\u001b[39;00m\n\u001b[1;32m     86\u001b[0m variance_result_gen \u001b[38;5;241m=\u001b[39m variance_filter_pipeline(data_factory(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvar_params)\n\u001b[0;32m---> 87\u001b[0m variance_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvariance_result_gen\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariance Output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariance_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# --- PHASE 2: FEATURE SELECTION (Run for all methods before DR) ---\u001b[39;00m\n",
                        "File \u001b[0;32m~/gk-lab/Automated-CV-Design/lda/feature_extraction/variance.py:225\u001b[0m, in \u001b[0;36mvariance_filter_pipeline\u001b[0;34m(df_iterator, return_threshold, show_plot, knee_S, outlier_multiplier, fallback_percentile, min_clean_ratio, plot_pause)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m variance_series \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_streaming_variance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_chunks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(variance_series) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: No features found for variance calculation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[0;32m~/gk-lab/Automated-CV-Design/lda/feature_extraction/variance.py:39\u001b[0m, in \u001b[0;36mcompute_streaming_variance\u001b[0;34m(df_iterator)\u001b[0m\n\u001b[1;32m     35\u001b[0m n_b \u001b[38;5;241m=\u001b[39m data_b\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_b \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m mean_b \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m m2_b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvar(data_b, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ddof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m n_b\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_a \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
                        "File \u001b[0;32m~/miniforge3/envs/gklab/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:3860\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3857\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3858\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3860\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3861\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/miniforge3/envs/gklab/lib/python3.10/site-packages/numpy/_core/_methods.py:137\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    135\u001b[0m ret \u001b[38;5;241m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 137\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrue_divide\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munsafe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_float16_result \u001b[38;5;129;01mand\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m         ret \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(ret)\n",
                        "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'int'"
                    ]
                }
            ],
            "source": [
                "print(\"ğŸš€ Starting Interactive Pipeline Runner\")\n",
                "print(\"ğŸ’¡ Variance runs first, then feature selection, then dimensionality reduction\")\n",
                "print(\"âš™ï¸  You'll set parameters for each phase\")\n",
                "\n",
                "# Use the pipeline runner\n",
                "results = run_interactive_pipeline(data_factory, configs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ğŸ“Š Successful pipelines: 0\n",
                        "âš ï¸  No pipelines completed successfully\n"
                    ]
                }
            ],
            "source": [
                "print(f\"ğŸ“Š Successful pipelines: {len(results)}\")\n",
                "\n",
                "if results:\n",
                "    print(\"\\nğŸ“ˆ Results Summary:\")\n",
                "    for pipeline_name, result in results.items():\n",
                "        final_df = result['final_result']\n",
                "        feature_cols = [col for col in final_df.columns if col != 'class']\n",
                "        \n",
                "        print(f\"   âœ… {pipeline_name}:\")\n",
                "        print(f\"      ğŸ“ Shape: {final_df.shape}\")\n",
                "        print(f\"      ğŸ”§ Features: {len(feature_cols)}\")\n",
                "        print(f\"      ğŸ·ï¸  Classes: {final_df['class'].nunique()}\")\n",
                "        \n",
                "        # Show feature names if not too many\n",
                "        if len(feature_cols) <= 5:\n",
                "            print(f\"      ğŸ“‹ Features: {feature_cols}\")\n",
                "        else:\n",
                "            print(f\"      ğŸ“‹ Features: {feature_cols[:3]}...{feature_cols[-2:]}\")\n",
                "        print()\n",
                "else:\n",
                "    print(\"âš ï¸  No pipelines completed successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âš ï¸  No results to save\n"
                    ]
                }
            ],
            "source": [
                "if results:\n",
                "    import pickle\n",
                "    import datetime\n",
                "    \n",
                "    # Create timestamp for filename\n",
                "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "    results_file = f\"pipeline_results_{timestamp}.pkl\"\n",
                "    \n",
                "    # Save results to file\n",
                "    with open(results_file, 'wb') as f:\n",
                "        pickle.dump(results, f)\n",
                "    \n",
                "    print(f\"ğŸ’¾ Results saved to: {results_file}\")\n",
                "    \n",
                "    # Also save summary to CSV\n",
                "    summary_data = []\n",
                "    for pipeline_name, result in results.items():\n",
                "        final_df = result['final_result']\n",
                "        summary_data.append({\n",
                "            'pipeline': pipeline_name,\n",
                "            'samples': final_df.shape[0],\n",
                "            'features': len([col for col in final_df.columns if col != 'class']),\n",
                "            'classes': final_df['class'].nunique(),\n",
                "            'feature_selection': result['config']['feature_selection'],\n",
                "            'dimensionality_reduction': result['config']['dimensionality_reduction']\n",
                "        })\n",
                "    \n",
                "    summary_df = pd.DataFrame(summary_data)\n",
                "    summary_file = f\"pipeline_summary_{timestamp}.csv\"\n",
                "    summary_df.to_csv(summary_file, index=False)\n",
                "    print(f\"ğŸ“Š Summary saved to: {summary_file}\")\n",
                "else:\n",
                "    print(\"âš ï¸  No results to save\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "gklab",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
